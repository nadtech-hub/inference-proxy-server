version: '3.8'
services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: inference-proxy-server
    ports:
      - "8080:8080"
    depends_on:
      - huggingfacetei
    environment:
      RUST_LOG: info
      MAX_WAIT_TIME_SEC: 10
      MAX_BATCH_SIZE: 5
      INFERENCE_SERVICE_URI: http://huggingfacetei:3000
    networks:
      - app-network

  huggingfacetei:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-latest
    command: --model-id nomic-ai/nomic-embed-text-v1.5
    ports:
      - "3000:3000"
    environment:
      - CORS_ALLOW_ORIGIN=http://0.0.0.0:8080
      - PORT=3000
      - MAX_CLIENT_BATCH_SIZE=512
    volumes:
      - ./embedding:/data
    networks:
      - app-network

networks:
  app-network:
    driver: bridge